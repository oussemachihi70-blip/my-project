{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkiyeFeYOaOx+CILTABadI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oussemachihi70-blip/my-projects/blob/main/malaria%20prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ClFZlkxoMuEa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from random import*\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "CfHqeuYpPYlw",
        "outputId": "2b8e1a76-6cf2-4ee7-8d7b-df21703a6636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the dataset"
      ],
      "metadata": {
        "id": "khiVDbwiN1cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"iarunava/cell-images-for-detecting-malaria\")"
      ],
      "metadata": {
        "id": "t7SwaMw0N4SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7db4654"
      },
      "source": [
        "# Task\n",
        "Preprocess the image data from the \"cell-images-for-detecting-malaria\" dataset located at `'/kaggle/input/cell-images-for-detecting-malaria'`, by loading images from the 'uninfected' and 'parasitized' subfolders, resizing them to a uniform size (e.g., 100x100 pixels), converting them into numerical arrays with corresponding labels, splitting the data into training and testing sets, and normalizing the pixel values. Finally, provide the shapes of `X_train`, `X_test`, `y_train`, and `y_test`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f7f9312"
      },
      "source": [
        "## Prepare Image Data\n",
        "\n",
        "### Subtask:\n",
        "Load images from the 'uninfected' and 'parasitized' subfolders, resize them to a uniform size (e.g., 100x100 pixels), convert them into numerical arrays, and create corresponding labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f43f40c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the specified path for X_train.txt was incorrect. The current subtask requires loading and processing image files directly from the dataset. Therefore, I need to implement the detailed instructions to load images, resize them, convert them to arrays, and create corresponding labels. I will start by importing the necessary libraries and then proceed with the image loading and preprocessing steps as outlined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eda4ca60"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the base directory for the dataset (from the kernel state)\n",
        "# path = '/kaggle/input/cell-images-for-detecting-malaria' # This variable is already defined and available\n",
        "\n",
        "# Construct the full paths to the 'uninfected' and 'parasitized' subfolders\n",
        "base_image_dir = os.path.join(path, 'cell_images', 'cell_images')\n",
        "uninfected_dir = os.path.join(base_image_dir, 'Uninfected')\n",
        "parasitized_dir = os.path.join(base_image_dir, 'Parasitized')\n",
        "\n",
        "# Initialize lists for storing image data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Define the target image size\n",
        "IMG_SIZE = 100\n",
        "\n",
        "# Iterate through each category and its corresponding label\n",
        "categories = {'Uninfected': 0, 'Parasitized': 1}\n",
        "\n",
        "for category_name, label in categories.items():\n",
        "    category_path = os.path.join(base_image_dir, category_name)\n",
        "    print(f\"Processing images in: {category_path}\")\n",
        "    for img_name in os.listdir(category_path):\n",
        "        try:\n",
        "            img_path = os.path.join(category_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                # Convert BGR to RGB\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                # Resize image\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                # Append to data and labels lists\n",
        "                data.append(img)\n",
        "                labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_name} in category {category_name}: {e}\")\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"Original data shape: {data.shape}\")\n",
        "print(f\"Original labels shape: {labels.shape}\")\n",
        "\n",
        "# Combine image data and labels into a single list of tuples\n",
        "combined_data = list(zip(data, labels))\n",
        "\n",
        "# Shuffle the combined list to ensure randomness\n",
        "random.shuffle(combined_data)\n",
        "\n",
        "# Separate the shuffled data back into X (images) and y (labels) NumPy arrays\n",
        "X = np.array([item[0] for item in combined_data])\n",
        "y = np.array([item[1] for item in combined_data])\n",
        "\n",
        "print(f\"Shuffled X shape: {X.shape}\")\n",
        "print(f\"Shuffled y shape: {y.shape}\")\n",
        "print(\"Image data loaded, processed, and labeled successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51aa04d3"
      },
      "source": [
        "## Split Data\n",
        "\n",
        "### Subtask:\n",
        "Split the preprocessed image data (`X`) and labels (`y`) into training and testing sets (e.g., `X_train`, `X_test`, `y_train`, `y_test`) to prepare for model training and evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14e7949d"
      },
      "source": [
        "**Reasoning**:\n",
        "To split the data into training and testing sets as per the subtask, I need to import the `train_test_split` function from `sklearn.model_selection` and then apply it to `X` and `y` with the specified parameters. Finally, I will print the shapes to verify the split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f3449bc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05b4e07f"
      },
      "source": [
        "## Normalize Image Data\n",
        "\n",
        "### Subtask:\n",
        "Normalize the pixel values of the image data (e.g., scale to a range of 0 to 1) to ensure consistent input for the neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8753705"
      },
      "source": [
        "**Reasoning**:\n",
        "To normalize the image data, I will convert the data types of `X_train` and `X_test` to 'float32' and then divide their pixel values by 255.0 to scale them to the range [0, 1]. Finally, I will print their shapes to confirm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18451964"
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "print(f\"Shape of normalized X_train: {X_train.shape}\")\n",
        "print(f\"Shape of normalized X_test: {X_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model"
      ],
      "metadata": {
        "id": "w726Iq5gUacI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(64,(3,3),input_shape=X_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64,(3,3),input_shape=X_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(X_train,y_train,batch_size=32,epochs=3,validation_split=0.1)"
      ],
      "metadata": {
        "id": "JOGuiUl_Uiuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "4nVDMSyuYDb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## make predictions"
      ],
      "metadata": {
        "id": "EudaV1B2cCst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx2=random.randint(0,len(X_test))\n",
        "img=X_test[idx2]\n",
        "plt.imshow(img)\n",
        "img=img.reshape(1,100,100,3)\n",
        "prediction=model.predict(img)\n",
        "if prediction>0.5:\n",
        "  print(\"parasitized\")\n",
        "else:\n",
        "  print(\"uninfected\")\n"
      ],
      "metadata": {
        "id": "QrWiC4SMcFXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}